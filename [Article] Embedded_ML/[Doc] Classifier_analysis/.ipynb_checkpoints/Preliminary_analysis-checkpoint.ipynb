{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b26e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import math\n",
    "import scipy.signal as signal\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def split_signal_into_windows(data, window_size, labels, threshold):\n",
    "    windows = []\n",
    "    windows_labels = []\n",
    "    num_samples = len(data)\n",
    "    num_windows = num_samples // window_size\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start = i * window_size\n",
    "        end = start + window_size\n",
    "        #process window\n",
    "        window = data[start:end]\n",
    "        windows.append(window)\n",
    "        #process label\n",
    "        label_series = labels[start:end]\n",
    "        anomaly_occurences = label_series.count(1)\n",
    "        \n",
    "        #get percentage of anomalous points\n",
    "        percentage = (anomaly_occurences/window_size) * 100\n",
    "        #return anomaly if percentage of anomalies is higher than threshold\n",
    "        #for a 0 threshold, if the window contains one anomalous point, it will be labeled as anomaly.\n",
    "        #for a 100 threshold, the window is labeled as anomaly only if all points were labeled as anomaly.\n",
    "        if (percentage > threshold):\n",
    "            windows_labels.append(1)\n",
    "        else:\n",
    "            windows_labels.append(0)\n",
    "\n",
    "        \n",
    "        \n",
    "    return windows, windows_labels\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        \n",
    "        data = []\n",
    "        for row in reader:\n",
    "            sample = [int(float(value)) for value in row[:]]\n",
    "            data.append(sample)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def plot_windows(windows):\n",
    "    num_windows = len(windows)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_windows, 1, figsize=(8, 4*num_windows))\n",
    "    \n",
    "    for i, window in enumerate(windows):\n",
    "        axes[i].plot(window)\n",
    "        axes[i].set_title(f'Window {i+1}')\n",
    "        axes[i].set_xlabel('Sample')\n",
    "        axes[i].set_ylabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def normalize_data(data):\n",
    "    normalized_data = []\n",
    "    num_channels = len(data[0])\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in data]\n",
    "        min_val = min(channel_data)\n",
    "        max_val = max(channel_data)\n",
    "        \n",
    "        normalized_channel = [(val - min_val) / (max_val - min_val) for val in channel_data]\n",
    "        normalized_data.append(normalized_channel)\n",
    "\n",
    "    normalized_data = np.transpose(normalized_data)\n",
    "    return normalized_data\n",
    "\n",
    "def split_data_and_labels(data):\n",
    "    num_channels = len(data[0])\n",
    "    channel_data = [sample[num_channels-1] for sample in data]\n",
    "    data = [sample[:-1] for sample in data]\n",
    "\n",
    "    return data, channel_data\n",
    "\n",
    "def calculate_channel_kurtosis(window):\n",
    "    num_channels = len(window[0])\n",
    "    kurtosis_values = []\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        channel_kurtosis = kurtosis(channel_data)\n",
    "        if np.isnan(channel_kurtosis):\n",
    "            channel_kurtosis = 0\n",
    "        kurtosis_values.append(channel_kurtosis)\n",
    "        \n",
    "    return kurtosis_values\n",
    "\n",
    "def calculate_channel_skewness(window):\n",
    "    num_channels = len(window[0])\n",
    "    skew_values = []\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        channel_skew = skew(channel_data)\n",
    "        if np.isnan(channel_skew):\n",
    "            channel_skew = 0\n",
    "        skew_values.append(channel_skew)\n",
    "        \n",
    "    return skew_values\n",
    "\n",
    "\n",
    "def calculate_channel_rms(window):\n",
    "    num_channels = len(window[0])\n",
    "    rms_values = []\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        rms = np.sqrt(np.mean(np.square(channel_data)))\n",
    "        if np.isnan(rms):\n",
    "            rms = 0\n",
    "        rms_values.append(rms)\n",
    "        \n",
    "    return rms_values\n",
    "\n",
    "def get_channels_peaks(window):\n",
    "    num_channels = len(window[0])\n",
    "    channel_peaks = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        peaks, _ = find_peaks(channel_data, distance=125)\n",
    "        var = np.var(peaks)\n",
    "\n",
    "        if np.isnan(var):\n",
    "            var = 0\n",
    "\n",
    "        channel_peaks.append(var)\n",
    "\n",
    "    return channel_peaks\n",
    "\n",
    "def get_zcp(window):\n",
    "    num_channels = len(window[0])\n",
    "    channel_peaks = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        mean_subtracted_signal = channel_data - np.mean(channel_data)\n",
    "\n",
    "        zero_crossings = np.where(np.diff(np.sign(mean_subtracted_signal)))[0]\n",
    "\n",
    "\n",
    "        channel_peaks.append(len(zero_crossings))\n",
    "\n",
    "    return channel_peaks\n",
    "\n",
    "def get_zcp_var(window):\n",
    "    num_channels = len(window[0])\n",
    "    channel_peaks = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        mean_subtracted_signal = channel_data - np.mean(channel_data)\n",
    "\n",
    "        zero_crossings = np.where(np.diff(np.sign(mean_subtracted_signal)))[0]\n",
    "        \n",
    "        var = np.var(zero_crossings)\n",
    "        if np.isnan(var):\n",
    "            var = 0\n",
    "        channel_peaks.append(var)\n",
    "\n",
    "    return channel_peaks\n",
    "\n",
    "def signal_entropy(window):\n",
    "    num_channels = len(window[0])\n",
    "    value_counts = {}\n",
    "    entropies = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        for value in channel_data:\n",
    "            if value in value_counts:\n",
    "                value_counts[value] += 1\n",
    "            else:\n",
    "                value_counts[value] = 1\n",
    "    \n",
    "        probabilities = [count / len(channel_data) for count in value_counts.values()]\n",
    "    \n",
    "        entropy_value = 0\n",
    "        for p in probabilities:\n",
    "            entropy_value -= p * math.log2(p)\n",
    "        entropies.append(entropy_value)\n",
    "    return entropies\n",
    "\n",
    "def compute_perfusion_index(window):\n",
    "    num_channels = len(window[0])\n",
    "    p_indexes = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        # Perform baseline removal to obtain pulsatile component\n",
    "        baseline = signal.medfilt(channel_data, kernel_size=301)\n",
    "        pulsatile_component = channel_data - baseline\n",
    "    \n",
    "        # Compute perfusion index as the ratio of pulsatile amplitude to the mean signal amplitude\n",
    "        perfusion_index = np.max(pulsatile_component) / np.mean(channel_data) * 100\n",
    "        p_indexes.append(perfusion_index)\n",
    "    return p_indexes\n",
    "\n",
    "def estimate_snr(window):\n",
    "    num_channels = len(window[0])\n",
    "    snrs = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        signal_power = np.var(channel_data)\n",
    "    \n",
    "        # Estimate the noise power using a portion of the signal assumed to be noise-free\n",
    "        noise_estimate = np.mean(channel_data[:100])  # Assuming the first 100 samples are noise-free\n",
    "        noise_power = np.var(channel_data - noise_estimate)\n",
    "    \n",
    "        snr = 10 * np.log10(signal_power / noise_power)\n",
    "        if np.isnan(snr):\n",
    "            snr = 0\n",
    "        snrs.append(snr)\n",
    "    return snrs\n",
    "\n",
    "def compute_mad(window):\n",
    "    num_channels = len(window[0])\n",
    "    mads = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        mad = np.median(np.abs(channel_data - np.median(channel_data)))\n",
    "        mads.append(mad)\n",
    "    return mads\n",
    "\n",
    "def compute_var(window):\n",
    "    num_channels = len(window[0])\n",
    "    varr = []\n",
    "    for channel in range(num_channels):\n",
    "        channel_data = [sample[channel] for sample in window]\n",
    "        var = np.var(channel_data)\n",
    "        varr.append(var)\n",
    "    return varr\n",
    "\n",
    "folder_path = '.'  # Path to the folder containing the files\n",
    "window_size = [125, 125 * 2, 125 * 3, 125 *4, 125*5, 125*6]\n",
    "\n",
    "#channels = [1, 2, 3]\n",
    "#window_size = [125 * 2]\n",
    "channels = [3]\n",
    "# Example usage\n",
    "all_windows = []\n",
    "all_labels = []\n",
    "use_selected_features = False\n",
    "shufle_input_data = True\n",
    "plot_tsne = False\n",
    "average_iterations = 5\n",
    "selected_features = np.array([True, True, True, False, False, False, False, False, False, True, True, True, False, True, True, True, True, \n",
    "                              True, False, True, False, False, False, False, True, False, True, True, False, False])\n",
    "sc = StandardScaler()\n",
    "\n",
    "for win_index, window_len in enumerate(window_size):\n",
    "    for no_of_channels in channels:\n",
    "        all_windows = []\n",
    "        all_labels = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path) and (filename.lower().endswith('.txt') or filename.lower().endswith('.csv')):\n",
    "            # Read the file\n",
    "                #print(file_path)\n",
    "                data = read_file(file_path)\n",
    "                data, labels = split_data_and_labels(data)\n",
    "                data = normalize_data(data)\n",
    "                # Split the signal into windows\n",
    "                windows, windows_labels = split_signal_into_windows(data, window_len, labels, 0)\n",
    "                for i, window in enumerate(windows):\n",
    "                    kurtosis_values = calculate_channel_kurtosis(window)\n",
    "                    skewness_values = calculate_channel_skewness(window)\n",
    "                    peaks_no = get_channels_peaks(window)\n",
    "                    zcp_no = get_zcp(window)\n",
    "                    zcp_var = get_zcp_var(window)\n",
    "                    ent = signal_entropy(window)\n",
    "                    pi = compute_perfusion_index(window)\n",
    "                    rms = calculate_channel_rms(window)\n",
    "                    mad = compute_mad(window)\n",
    "                    var = compute_var(window)\n",
    "                    features = np.concatenate((kurtosis_values[0:no_of_channels], skewness_values[0:no_of_channels], peaks_no[0:no_of_channels],\n",
    "                                               zcp_no[0:no_of_channels], zcp_var[0:no_of_channels], ent[0:no_of_channels], pi[0:no_of_channels],\n",
    "                                               rms[0:no_of_channels], mad[0:no_of_channels], var[0:no_of_channels]))\n",
    "                    all_windows.append(features)\n",
    "                    all_labels.append(windows_labels[i])\n",
    "            #print(len(all_windows))\n",
    "        all_windows = sc.fit_transform(all_windows)\n",
    "\n",
    "        # Combine the arrays into a list of tuples\n",
    "        combined = list(zip(all_windows, all_labels))\n",
    "        # Shuffle the combined list\n",
    "        accuracy_1 = accuracy_2 = accuracy_3 = accuracy_4 = 0\n",
    "        f1_1 = f1_2 = f1_3 = f1_4 = 0\n",
    "        \n",
    "\n",
    "        for i in range(0,average_iterations):\n",
    "            if shufle_input_data:\n",
    "                random.shuffle(combined)\n",
    "            # Unzip the shuffled list back into separate arrays\n",
    "            all_windows_sh, all_labels_sh = zip(*combined)\n",
    "            data_list = list(all_windows_sh)\n",
    "            new_data = []\n",
    "            for entry in data_list:\n",
    "                new_entry = [entry[i] for i in range(0, len(entry)) if selected_features[i] == True]\n",
    "                new_data.append(new_entry)\n",
    "            if use_selected_features:\n",
    "                all_windows_sh = new_data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(all_windows_sh, all_labels_sh, test_size=0.2)\n",
    "            \n",
    "            X_train = list(np.nan_to_num(X_train, nan=0.0))\n",
    "            X_test = list(np.nan_to_num(X_test, nan=0.0))\n",
    "            \n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(30), max_iter=30,activation = 'relu',\n",
    "                                     solver='adam',random_state=1)\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred_4 = classifier.predict(X_test)\n",
    "            accuracy_4 += accuracy_score(y_test, y_pred_4)\n",
    "            f1_4 += f1_score(y_test, y_pred_4)\n",
    "        #plt.plot(classifier.loss_curve_)\n",
    "        #plt.plot(classifier.validation_scores_)\n",
    "            svc = SVC(C = 1, gamma = 1, kernel = 'linear', probability=True)\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_pred_3 = svc.predict(X_test)\n",
    "            accuracy_3 += accuracy_score(y_test, y_pred_3)\n",
    "            f1_3 += f1_score(y_test, y_pred_3)\n",
    "#\n",
    "            dt = DecisionTreeClassifier(criterion= 'gini', max_depth=6)\n",
    "            dt.fit(X_train, y_train)\n",
    "            y_pred_1 = dt.predict(X_test)\n",
    "            accuracy_1 += accuracy_score(y_test, y_pred_1)\n",
    "            f1_1 += f1_score(y_test, y_pred_1)\n",
    "            \n",
    "            rf = RandomForestClassifier(max_depth = 5, n_estimators = 15)\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred_2 = rf.predict(X_test)\n",
    "            accuracy_2 += accuracy_score(y_test, y_pred_2)\n",
    "            f1_2 += f1_score(y_test, y_pred_2)\n",
    "            \n",
    "            #print(no_of_channels, accuracy_1/(i + 1), accuracy_2/(i + 1), accuracy_3/(i + 1), accuracy_4/(i + 1))\n",
    "            \n",
    "        print(\"Accuracy DT:\", accuracy_1/average_iterations, window_len, no_of_channels)\n",
    "        print(\"F1 DT:\", f1_1/average_iterations, window_len, no_of_channels)\n",
    "        print(\"Accuracy RF:\", accuracy_2/average_iterations, window_len, no_of_channels)\n",
    "        print(\"F1 RF:\", f1_2/average_iterations, window_len, no_of_channels)\n",
    "        print(\"Accuracy SVC:\", accuracy_3/average_iterations, window_len, no_of_channels)\n",
    "        print(\"F1 SVC:\", f1_3/average_iterations, window_len, no_of_channels)\n",
    "        print(\"Accuracy MLP:\", accuracy_4/average_iterations, window_len, no_of_channels)\n",
    "        print(\"F1 MLP:\", f1_4/average_iterations, window_len, no_of_channels)\n",
    "        data_list = list(all_windows)\n",
    "        \n",
    "        models = [dt, rf, svc, classifier]\n",
    "        y_scores = classifier.predict_proba(X_test)[:, 1]\n",
    "        # Compute the false positive rate (FPR), true positive rate (TPR), and thresholds\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "        \n",
    "        # Compute the AUC score\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot the ROC curve\n",
    "        plt.plot(fpr, tpr, label='w{} ROC Curve (AUC = {:.2f})'.format(win_index + 1, auc_score))\n",
    "\n",
    "\n",
    "        if plot_tsne:\n",
    "            class_labels = np.array(all_labels_sh)  # Convert to NumPy array if needed\n",
    "            unique_classes = np.unique(class_labels)  # Get unique class labels\n",
    "            num_classes = len(windows_labels)\n",
    "            tsne = TSNE(n_components=3, perplexity=50)\n",
    "            embedded_data = tsne.fit_transform(np.array(new_data))\n",
    "        \n",
    "        \n",
    "            colors = ['r', 'b']  # Generate a range of colors\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            labels = ['Clean windows', 'Anomalous windows']\n",
    "            for i, u in enumerate(unique_classes):\n",
    "                class_data = embedded_data[class_labels == u]\n",
    "                ax.scatter(class_data[:, 0], class_data[:, 1],  class_data[:, 2], c=colors[i], label=labels[i])\n",
    "            \n",
    "            ax.set_xlabel('tSNE-1')\n",
    "            ax.set_ylabel('tSNE-2')\n",
    "            ax.set_title('tSNE visualization of extracted features from PPG signals')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('MLP Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792845e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Feature Analysis\n",
    "# Perform statistical analysis on the features (e.g., correlations, distributions)\n",
    "\n",
    "# Step 3: Feature Selection\n",
    "# SelectKBest with mutual information as an example\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=30)  # Select top 15 features\n",
    "X_selected = selector.fit_transform(all_windows, all_labels)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the corresponding feature names\n",
    "\n",
    "# Plot the feature importance scores\n",
    "scores = selector.scores_\n",
    "plt.bar(range(len(scores)), scores)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Scores')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Cross-Validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train and Evaluate Classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(selected_feature_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of logistic regression\n",
    "logreg = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize variables to store the best performance\n",
    "best_accuracy = 0\n",
    "best_f1 = 0\n",
    "best_features = None\n",
    "\n",
    "# Iterate over different numbers of features\n",
    "for n in range(1, len(X_train[0])+1):\n",
    "    # Create an RFE model with logistic regression and specify the number of features to select\n",
    "    rfe = RFE(estimator=logreg, n_features_to_select=n)\n",
    "\n",
    "    # Fit the RFE model to your training data\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    # Access the selected features\n",
    "    selected_features = rfe.support_\n",
    "\n",
    "    # Filter the data based on selected features\n",
    "    new_data = [entry[selected_features] for entry in all_windows_sh]\n",
    "\n",
    "    # Split the filtered data into training and test sets\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(new_data, all_labels_sh, test_size=0.2)\n",
    "    print(len(X_train_2[0]))\n",
    "    # Evaluate the performance of the random forest classifier\n",
    "    accuracy = 0\n",
    "    f1 = 0\n",
    "    for _ in range(10):\n",
    "        rf = DecisionTreeClassifier()\n",
    "        rf.fit(X_train_2, y_train_2)\n",
    "        y_pred_2 = rf.predict(X_test_2)\n",
    "        accuracy += accuracy_score(y_test_2, y_pred_2)\n",
    "        f1 += f1_score(y_test_2, y_pred_2)\n",
    "\n",
    "    # Calculate average performance\n",
    "    accuracy /= 10\n",
    "    f1 /= 10\n",
    "    print(\"Accuracy RF:\", accuracy)\n",
    "    print(\"F1 RF:\", f1)\n",
    "    # Check if the current performance is better than the previous best\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_f1 = f1\n",
    "        best_features = selected_features\n",
    "\n",
    "# Print the best performance and selected features\n",
    "print(\"Best Accuracy RF:\", best_accuracy)\n",
    "print(\"Best F1 RF:\", best_f1)\n",
    "print(\"Best Features:\", best_features, len(np.where(best_features)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b86e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# Initialize\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=100,activation = 'relu',\n",
    "                    solver='adam',random_state=1, learning_rate_init=1e-3)\n",
    "batch_size, train_loss_, valid_loss_ = 50, [], []\n",
    "# Training Loop\n",
    "for _ in range(50):\n",
    "    for b in range(batch_size, len(y_train), batch_size):\n",
    "    \n",
    "        X_batch, y_batch = X_train[b-batch_size:b], y_train[b-batch_size:b]\n",
    "\n",
    "        clf.partial_fit(X_batch, y_batch, classes=[0, 1])\n",
    "        train_loss_.append(clf.loss_)\n",
    "        valid_loss_.append(log_loss(y_test, clf.predict_proba(X_test)))\n",
    "\n",
    "plt.plot(range(len(train_loss_)), train_loss_, label=\"train loss\")\n",
    "plt.plot(range(len(train_loss_)), valid_loss_, label=\"validation loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844dafac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Print feature importances\n",
    "#for i, importance in enumerate(feature_importances):\n",
    "#    print(f\"Channel {(i%3)} Importance: {importance}\")\n",
    "    \n",
    "channel_labels = [f\"Ch{i+1}\" for i in range(len(feature_importances))]\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.bar(channel_labels, feature_importances)\n",
    "plt.xlabel('Channels')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance based on Kurtosis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a880cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(all_windows, all_labels)\n",
    "\n",
    "# Get the feature importance scores\n",
    "importance_scores = model.feature_importances_\n",
    "\n",
    "# Print the feature importance scores\n",
    "#for i, score in enumerate(importance_scores):\n",
    "channel_labels = [f\"Ch{i+1}\" for i in range(len(importance_scores))]\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.bar(channel_labels, importance_scores)\n",
    "plt.xlabel('Channels')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance based on Kurtosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterSamples(normal_samples, anomaly_samples, num_samples, title=''):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    for i in range(num_samples):\n",
    "        ax.scatter(normal_samples[i].T[0], normal_samples[i].T[1], normal_samples[i].T[2], c='b')\n",
    "        ax.scatter(anomaly_samples[i].T[0], anomaly_samples[i].T[1], anomaly_samples[i].T[2], c='r')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5de251",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [i for i, x in enumerate(all_labels) if x == 0]\n",
    "# Convert the tuple to a list\n",
    "data_list = list(all_windows)\n",
    "# Index the list using the indexes list\n",
    "selected_values_normal = [data_list[i] for i in indexes]\n",
    "\n",
    "indexes = [i for i, x in enumerate(all_labels_sh) if x == 1]\n",
    "# Index the list using the indexes list\n",
    "selected_values_abnormal = [data_list[i] for i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(0, 9):\n",
    "    plot_normal = [arr[j:j+3] for arr in selected_values_normal]\n",
    "    plot_abnormal = [arr[j:j+3] for arr in selected_values_abnormal]\n",
    "    plotScatterSamples(plot_normal, plot_abnormal,len(plot_abnormal))\n",
    "    j = (i + 1) * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "param_dist = {'n_estimators': [5, 10, 15],\n",
    "              'max_depth': [1, 2, 3, 4, 5, 6]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_windows_sh, all_labels_sh, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = GridSearchCV(rf, param_grid = param_dist, cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = rand_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy, window_len, no_of_channels)\n",
    "# Print the best hyperparameters\n",
    "print('RF Best hyperparameters:',  rand_search.best_params_, rand_search.best_score_)\n",
    "\n",
    "param_dist = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [1, 2, 3, 4, 5, 6]}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = GridSearchCV(dt, param_grid = param_dist, cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = rand_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy, window_len, no_of_channels)\n",
    "# Print the best hyperparameters\n",
    "print('DT Best hyperparameters:',  rand_search.best_params_, rand_search.best_score_)\n",
    "\n",
    "param_grid = {'C':[1,10],'gamma':[1,0.1, 0.001], 'kernel':['linear','rbf']}\n",
    "\n",
    "dt = SVC()\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = GridSearchCV(dt, param_grid = param_grid, cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = rand_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy, window_len, no_of_channels)\n",
    "# Print the best hyperparameters\n",
    "print('SVC Best hyperparameters:',  rand_search.best_params_, rand_search.best_score_)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
